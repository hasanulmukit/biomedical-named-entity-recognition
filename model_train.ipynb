from datasets import load_dataset
dataset = load_dataset("bigbio/jnlpba", "jnlpba_bigbio_kb")  # NER for DNA, RNA, protein, etc. :contentReference[oaicite:4]{index=4}

def to_iob(data):
    # read tokens and BIO tags, return list of dicts: {"tokens": [...], "tags": [...]}
    ...

split = dataset["train"].train_test_split(test_size=0.1, seed=42)
train_ds, val_ds = split["train"], split["test"]

from transformers import AutoTokenizer, AutoModelForTokenClassification
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1")
model = AutoModelForTokenClassification.from_pretrained("dmis-lab/biobert-base-cased-v1.1", num_labels=num_tags)

tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
model = AutoModelForTokenClassification.from_pretrained("allenai/scibert_scivocab_uncased", num_labels=num_tags)

def tokenize_and_align_labels(examples):
    tokenized = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized.word_ids(batch_index=i)
        aligned = []
        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                aligned.append(-100)
            elif word_idx != previous_word_idx:
                aligned.append(label[word_idx])
            else:
                aligned.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx
        labels.append(aligned)
    tokenized["labels"] = labels
    return tokenized

train_tokenized = train_ds.map(tokenize_and_align_labels, batched=True)
val_tokenized   = val_ds.map(tokenize_and_align_labels, batched=True)

from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="outputs",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer,
)
trainer.train()

from seqeval.metrics import classification_report

predictions, labels, _ = trainer.predict(val_tokenized)
pred_tags = [
    [label_list[p] for (p, l) in zip(pred, lab) if l != -100]
    for pred, lab in zip(predictions.argmax(-1), labels)
]
true_tags = [
    [label_list[l] for (p, l) in zip(pred, lab) if l != -100]
    for pred, lab in zip(predictions.argmax(-1), labels)
]
print(classification_report(true_tags, pred_tags))
